# Real Estate Data Scraping Application
A real estate data scrapping application that scrapes data from BAYUT a prominent real estate website in UAE.This project is a Python web scraper that extracts real estate data from the Realtor.com website for the city of Stockton, California. The data is then saved to either a sqlite3 database or a CSV file, depending on the file that is run.
## Getting started
To run the web scraper, you will need to have Python 3 installed on your machine. You will also need to install the following Python libraries:
- BeautifulSoup
- requests
- selenium
  
You can install these libraries by running the following command in your terminal: 'pip install beautifulsoup4 requests mysql-connector-python'
And also you need to add cron tabs.
## How to Use
To utilize the web scraper, begin by executing one of the two source code files provided: bayutscraping1.py and propertyinformation.py. These files are responsible for scraping data from online sources. Once executed, they extract property information from the specified websites.

The scraped data can be stored in a CSV file. To accomplish this, ensure that the CSV files generated by the scraper are moved to the designated directory within the media folder. Within the media folder, there should be a subfolder named csvfiles where the CSV files are to be stored.

Following the data extraction and storage in CSV format, a separate process handles the data insertion into an SQLite3 database. This process is managed by the cron.py file, which serves as a cron job. It periodically retrieves the CSV files from the csvfiles folder within the media directory and inserts their contents into the SQLite3 database.
## Data Collected
- Apartment_Detail
- Building_Information
- Validated_Information
- Project_Information
- Property_Detail
## Credits
This project was created by computingfuturetech. The code is based on examples from the Beautiful Soup documentation and the requests,selenium library documentation.
